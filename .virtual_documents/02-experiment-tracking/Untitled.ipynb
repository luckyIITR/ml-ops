import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import root_mean_squared_error, mean_squared_error
from sklearn import ensemble
import pickle


import mlflow
mlflow.set_tracking_uri('sqlite:///mlflow.db')
mlflow.set_experiment('nyc-taxi-experiment')


df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet')


df.head()


df = df[:10000].copy()


df['duration'] = (df.tpep_dropoff_datetime - df.tpep_pickup_datetime).apply(lambda tz: tz.seconds / 60)


df = df[((df.duration >= 1) & (df.duration <= 60))]


categorical = ['PULocationID', 'DOLocationID']
numerical = ['trip_distance']


train_dicts = df[categorical + numerical].to_dict(orient = 'records')

dv = DictVectorizer()
X = dv.fit_transform(train_dicts)


target = 'duration'
y = df[target].values


X_train, X_val, y_train, y_val = train_test_split(X, y)


mlflow.autolog()


params = {
    "n_estimators": 500,
    "max_depth": 4,
    "min_samples_split": 5,
    "learning_rate": 0.01,
    "loss": "squared_error",
}


reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_train, y_train)

mse = mean_squared_error(y_val, reg.predict(X_val))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse))


from hyperopt import hp
from hyperopt import fmin, tpe, space_eval, STATUS_OK, Trials
from hyperopt.pyll import scope


# define an objective function
def objective(params):
    reg = ensemble.GradientBoostingRegressor(**params)
    reg.fit(X_train, y_train)
    mse = mean_squared_error(y_val, reg.predict(X_val))
    return {'loss': mse, 'status': STATUS_OK}
    
# define a search space
search_space = {
    "n_estimators": 500,
    "min_samples_split": 5,
    "loss": "squared_error",
    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),
    'learning_rate': hp.loguniform('learning_rate', -3, 0)
}

# minimize the objective over the space

best = fmin(objective, search_space, algo=tpe.suggest, max_evals=100, trials=Trials())

print(best)
# -> {'a': 1, 'c2': 0.01420615366247227}
print(space_eval(space, best))
# -> ('case 2', 0.01420615366247227}





params = {
    "n_estimators": 500,
    "max_depth": 5,
    "min_samples_split": 5,
    "learning_rate": 0.06657731945633238,
    "loss": "squared_error",
}
reg = ensemble.GradientBoostingRegressor(**params)
reg.fit(X_train, y_train)

mse = mean_squared_error(y_val, reg.predict(X_val))
print("The mean squared error (MSE) on test set: {:.4f}".format(mse))






